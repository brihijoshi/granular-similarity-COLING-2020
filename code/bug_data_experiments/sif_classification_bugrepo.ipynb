{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Snap Inc. 2020. This sample code is made available by Snap Inc. for informational purposes only. It is provided as-is, without warranty of any kind, express or implied, including any warranties of merchantability, fitness for a particular purpose, or non-infringement. In no event will Snap Inc. be liable for any damages arising from the sample code or your use thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import hashlib\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from xgboost import XGBClassifier\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from tf_idf_vectorizer import *\n",
    "from utils.snap_preprocessed_df_handle import *\n",
    "from utils.EstimatorSelectionHelper import EstimatorSelectionHelper\n",
    "from utils.classifier_setup import *\n",
    "\n",
    "# SIF Classification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_tf_idf(PATH):\n",
    "    df_with_keywords = get_dataframe(PATH)\n",
    "    return df_with_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../data/dataframes/df_train_bugrepo_eclipse.pkl'\n",
    "TEST_PATH = '../../data/dataframes/df_test_bugrepo_eclipse.pkl'\n",
    "train_df = pd.read_pickle(TRAIN_PATH)\n",
    "test_df = pd.read_pickle(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train = pd.concat([train_df[['id1','title1']].\\\n",
    "                            rename(columns={'id1':'id','title1':'title'}), \\\n",
    "                            train_df[['id2','title2']].\\\n",
    "                            rename(columns={'id2':'id','title2':'title'})]\n",
    "                          ).drop_duplicates().reset_index(drop=True)\n",
    "non_dup_articles_train = articles_train['id'].drop_duplicates().index\n",
    "articles_train = articles_train.loc[non_dup_articles_train].reset_index(drop=True)\n",
    "\n",
    "articles_test = pd.concat([test_df[['id1','title1']].\\\n",
    "                            rename(columns={'id1':'id','title1':'title'}), \\\n",
    "                            test_df[['id2','title2']].\\\n",
    "                            rename(columns={'id2':'id','title2':'title'})]\n",
    "                          ).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "non_dup_articles_test = articles_test['id'].drop_duplicates().index\n",
    "articles_test = articles_test.loc[non_dup_articles_test].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../SIF/src/')\n",
    "import data_io, params, SIF_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfile = '../../data/pretrained/glove.840B.300d.txt' # word vector file, can be downloaded from GloVe website\n",
    "weightfile = '../SIF/auxiliary_data/enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    regex = '(?<!\\d)[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~](?!\\d)'\n",
    "    return re.sub(regex, \"\", text, 0).lower()\n",
    "\n",
    "def getWordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = open(textfile,'r')\n",
    "    lines = f.readlines()\n",
    "    for (n,i) in enumerate(lines):\n",
    "        i=i.split(\" \")\n",
    "        j = 1\n",
    "        v = []\n",
    "        try:\n",
    "            while j < len(i):\n",
    "                v.append(float(i[j]))\n",
    "                j += 1\n",
    "            words[i[0]]=n\n",
    "            We.append(v)\n",
    "        except:\n",
    "            print('Not working for - ',i[0])\n",
    "    return (words, np.array(We))\n",
    "\n",
    "def getWeight(words, word2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in word2weight:\n",
    "            weight4ind[ind] = word2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind\n",
    "\n",
    "def getWordWeight(weightfile, a=1e-3):\n",
    "    if a <=0: # when the parameter makes no sense, use unweighted\n",
    "        a = 1.0\n",
    "\n",
    "    word2weight = {}\n",
    "    with open(weightfile) as f:\n",
    "        lines = f.readlines()\n",
    "    N = 0\n",
    "    for i in lines:\n",
    "        i=i.strip()\n",
    "        if(len(i) > 0):\n",
    "            i=i.split()\n",
    "            if(len(i) == 2):\n",
    "                word2weight[i[0]] = float(i[1])\n",
    "                N += float(i[1])\n",
    "            else:\n",
    "                print(i)\n",
    "    for key, value in word2weight.items():\n",
    "        word2weight[key] = a / (a + value/N)\n",
    "    return word2weight\n",
    "\n",
    "def sentences2idx(sentences, words):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    seq1 = []\n",
    "    for i in sentences:\n",
    "        seq1.append(data_io.getSeq(i,words))\n",
    "    x1,m1 = data_io.prepare_data(seq1)\n",
    "    return x1, m1\n",
    "\n",
    "def seq2weight(seq, mask, weight4ind):\n",
    "    weight = np.zeros(seq.shape).astype('float32')\n",
    "    for i in range(seq.shape[0]):\n",
    "        for j in range(seq.shape[1]):\n",
    "            if mask[i,j] > 0 and seq[i,j] >= 0:\n",
    "                weight[i,j] = weight4ind[seq[i,j]]\n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def get_weighted_average(We, x, w):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    emb = np.zeros((n_samples, We.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        emb[i,:] = w[i,:].dot(We[x[i,:],:]) / np.count_nonzero(w[i,:])\n",
    "    return emb\n",
    "\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "\n",
    "def interpolate_nans(X):\n",
    "    \"\"\"Overwrite NaNs with column value interpolations.\"\"\"\n",
    "    for j in range(X.shape[1]):\n",
    "        mask_j = np.isnan(X[:,j])\n",
    "        X[mask_j,j] = np.interp(np.flatnonzero(mask_j), np.flatnonzero(~mask_j), X[~mask_j,j])\n",
    "    return X\n",
    "\n",
    "def SIF_embedding(We, x, w, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = get_weighted_average(We, x, w)\n",
    "    emb = interpolate_nans(emb)\n",
    "    if  params.rmpc > 0:\n",
    "        emb = remove_pc(emb, params.rmpc)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = list(articles_train['title'].apply(preprocessor))\n",
    "(words, We) = getWordmap(wordfile)\n",
    "# load word weights\n",
    "word2weight = getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
    "# load sentences\n",
    "x, m = sentences2idx(sentences_train, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = seq2weight(x, m, weight4ind) # get word weights\n",
    "param = params.params()\n",
    "param.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding(We, x, w, param) # embedding[i,:] is the embedding for sentence i\n",
    "embedding_train = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = list(articles_test['title'].apply(preprocessor))\n",
    "x, m = sentences2idx(sentences_test, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = seq2weight(x, m, weight4ind) # get word weights\n",
    "param = params.params()\n",
    "param.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding(We, x, w, param) # embedding[i,:] is the embedding for sentence i\n",
    "embedding_test = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train['index'] = articles_train.index\n",
    "articles_test['index'] = articles_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train['embed'] = articles_train['index'].apply(lambda x: embedding_train[x])\n",
    "articles_test['embed'] = articles_test['index'].apply(lambda x: embedding_test[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sif_embedding1'] = train_df['id1'].\\\n",
    "                                apply(lambda x: articles_train[articles_train['id']==x]\\\n",
    "                                      ['embed'].values[0])\n",
    "train_df['sif_embedding2'] = train_df['id2'].\\\n",
    "                                apply(lambda x: articles_train[articles_train['id']==x]\\\n",
    "                                      ['embed'].values[0])\n",
    "train_df['sif_similarity'] = train_df[['sif_embedding1','sif_embedding2']]\\\n",
    "                                        .apply(lambda row: \\\n",
    "                                               cosine_similarity(row['sif_embedding1'].reshape(1, -1),\\\n",
    "                                                                 row['sif_embedding2'].reshape(1, -1))[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sif_embedding1'] = test_df['id1'].\\\n",
    "                                apply(lambda x: articles_test[articles_test['id']==x]\\\n",
    "                                      ['embed'].values[0])\n",
    "test_df['sif_embedding2'] = test_df['id2'].\\\n",
    "                                apply(lambda x: articles_test[articles_test['id']==x]\\\n",
    "                                      ['embed'].values[0])\n",
    "test_df['sif_similarity'] = train_df[['sif_embedding1','sif_embedding2']]\\\n",
    "                                        .apply(lambda row: \\\n",
    "                                               cosine_similarity(row['sif_embedding1'].reshape(1, -1),\\\n",
    "                                                                 row['sif_embedding2'].reshape(1, -1))[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_pickle('../../data/dataframes/df_train_bugrepo_sif_similarity.pkl')\n",
    "# test_df.to_pickle('../../data/dataframes/df_test_bugrepo_sif_similarity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "           \"XGBoost\" : XGBClassifier()\n",
    "}\n",
    "\n",
    "params = {'XGBoost':  {\"colsample_bytree\": [0.3,0.5,0.8,1],\"gamma\":[0,10,50,100],\n",
    "                        \"max_depth\": [2,4,6], # default 3\\\n",
    "                        \"n_estimators\": [50,100], # default 100\n",
    "                        \"subsample\": [0.3,0.5,0.8,1]}\n",
    "}\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_event = EstimatorSelectionHelper(models, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_event.fit(train_df['sif_similarity'].values.reshape(-1, 1),\n",
    "            train_df['dup_issue'],\n",
    "            cv = 5,\n",
    "            scoring=make_scorer(custom_scorer, greater_is_better=True), n_jobs=16, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helper_event.summary(test_df['sif_similarity'], test_df['dup_issue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper_event.save_models('../../data/models/', 'bugrepo_sif_event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dup_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_topic = EstimatorSelectionHelper(models, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_topic.fit(train_df['sif_similarity'].values.reshape(-1, 1),\n",
    "            train_df['dup_group'],\n",
    "            cv = 5,\n",
    "            scoring=make_scorer(custom_scorer, greater_is_better=True), n_jobs=16, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helper_topic.summary(test_df['sif_similarity'], test_df['dup_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper_event.save_models('../../data/models/', 'bugrepo_sif_topic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

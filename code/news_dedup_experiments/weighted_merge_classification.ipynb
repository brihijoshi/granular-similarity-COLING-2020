{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Snap Inc. 2020. This sample code is made available by Snap Inc. for informational purposes only. It is provided as-is, without warranty of any kind, express or implied, including any warranties of merchantability, fitness for a particular purpose, or non-infringement. In no event will Snap Inc. be liable for any damages arising from the sample code or your use thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import hashlib\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from xgboost import XGBClassifier\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from tf_idf_vectorizer import *\n",
    "from utils.snap_preprocessed_df_handle import *\n",
    "from utils.EstimatorSelectionHelper import EstimatorSelectionHelper\n",
    "from utils.classifier_setup import *\n",
    "\n",
    "# BERT Classification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../data/dataframes/df_unique_with_similarity.pkl'\n",
    "TEST_PATH = '../../data/dataframes/df_test_unique_with_similarity.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_tf_idf(PATH):\n",
    "    df_with_keywords = get_dataframe(PATH)\n",
    "    articles = get_unique_combined_with_id(df_with_keywords, 'Input.article', 'article')\n",
    "    od_output, od_keys = get_tf_idf(articles, 'article', preprocessor=preprocessor, stop_words=stop_list, ngram_range = (1,1))\n",
    "    df_with_keywords['tfidf_v1'] = df_with_keywords['id1'].apply(lambda x: od_output[list(od_keys).index(x)])\n",
    "    df_with_keywords['tfidf_v2'] = df_with_keywords['id2'].apply(lambda x: od_output[list(od_keys).index(x)])\n",
    "    df_with_keywords['tfidf_similarity'] = df_with_keywords[['tfidf_v1','tfidf_v2']]\\\n",
    "                                        .apply(lambda row: cosine_similarity(row['tfidf_v1'],row['tfidf_v2'])[0][0], axis=1)\n",
    "    \n",
    "    return df_with_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_dataframe_tf_idf(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_dataframe_tf_idf(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['k1','k2','textrank_similarity','tfidf_v1','tfidf_v2'], inplace=True)\n",
    "test_df.drop(columns=['k1','k2','textrank_similarity','tfidf_v1','tfidf_v2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train = pd.concat([train_df[['id1','Input.article1']].\\\n",
    "                            rename(columns={'id1':'id','Input.article1':'article'}), \\\n",
    "                            train_df[['id2','Input.article2']].\\\n",
    "                            rename(columns={'id2':'id','Input.article2':'article'})]\n",
    "                          ).drop_duplicates().reset_index(drop=True)\n",
    "non_dup_articles_train = articles_train['id'].drop_duplicates().index\n",
    "articles_train = articles_train.loc[non_dup_articles_train].reset_index(drop=True)\n",
    "\n",
    "articles_test = pd.concat([test_df[['id1','Input.article1']].\\\n",
    "                            rename(columns={'id1':'id','Input.article1':'article'}), \\\n",
    "                            test_df[['id2','Input.article2']].\\\n",
    "                            rename(columns={'id2':'id','Input.article2':'article'})]\n",
    "                          ).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "non_dup_articles_test = articles_test['id'].drop_duplicates().index\n",
    "articles_test = articles_test.loc[non_dup_articles_test].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/dataframes/roberta_sentence_embeddings_train.pkl', 'rb') as f:\n",
    "    sentence_embeddings_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/dataframes/roberta_sentence_embeddings_test.pkl', 'rb') as f:\n",
    "    sentence_embeddings_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train['roberta_embedding'] = sentence_embeddings_train\n",
    "articles_test['roberta_embedding'] = sentence_embeddings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['roberta_embedding1'] = train_df['id1'].\\\n",
    "                                apply(lambda x: articles_train[articles_train['id']==x]\\\n",
    "                                      ['roberta_embedding'].values[0])\n",
    "train_df['roberta_embedding2'] = train_df['id2'].\\\n",
    "                                apply(lambda x: articles_train[articles_train['id']==x]\\\n",
    "                                      ['roberta_embedding'].values[0])\n",
    "train_df['roberta_similarity'] = train_df[['roberta_embedding1','roberta_embedding2']]\\\n",
    "                                        .apply(lambda row: \\\n",
    "                                               cosine_similarity(row['roberta_embedding1'].reshape(1, -1),\\\n",
    "                                                                 row['roberta_embedding2'].reshape(1, -1))[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['roberta_embedding1'] = test_df['id1'].\\\n",
    "                                apply(lambda x: articles_test[articles_test['id']==x]\\\n",
    "                                      ['roberta_embedding'].values[0])\n",
    "test_df['roberta_embedding2'] = test_df['id2'].\\\n",
    "                                apply(lambda x: articles_test[articles_test['id']==x]\\\n",
    "                                      ['roberta_embedding'].values[0])\n",
    "test_df['roberta_similarity'] = test_df[['roberta_embedding1','roberta_embedding2']]\\\n",
    "                                        .apply(lambda row: \\\n",
    "                                               cosine_similarity(row['roberta_embedding1'].reshape(1, -1),\\\n",
    "                                                                 row['roberta_embedding2'].reshape(1, -1))[0][0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedTransform( BaseEstimator, TransformerMixin ):\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self, weight_factor=0.5):\n",
    "        self._weight_factor = weight_factor\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    #Transform method we wrote for this transformer \n",
    "    def transform(self, X, y = None):\n",
    "       #Depending on constructor argument break dates column into specified units\n",
    "       #using the helper functions written above \n",
    "        tf_idf_factor = self._weight_factor\n",
    "        bert_factor = 1-self._weight_factor\n",
    "        X['merged_similarity'] = tf_idf_factor * X['tfidf_similarity'] + bert_factor * X['roberta_similarity']\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelperWrapper(BaseEstimator):\n",
    "    #Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self, models, params, y_label='majority_same_event'):\n",
    "        self._helper = EstimatorSelectionHelper(models, params)\n",
    "        self.y_label = y_label\n",
    "        \n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y=None):\n",
    "        self._helper.fit(X['merged_similarity'].values.reshape(-1, 1),\n",
    "            X[self.y_label],\n",
    "            cv = 5,\n",
    "            scoring=make_scorer(custom_scorer, greater_is_better=True), n_jobs=16, refit=True)\n",
    "        return self\n",
    "\n",
    "    #Transform method we wrote for this transformer \n",
    "    def predict(self, X, y = None):\n",
    "       #Depending on constructor argument break dates column into specified units\n",
    "       #using the helper functions written above \n",
    "        self._helper.summary(X['merged_similarity'], X[self.y_label])\n",
    "        \n",
    "    def save_models(self,path,name):\n",
    "        self._helper.save_models(path,name)\n",
    "        \n",
    "    def save_helper(self, path, name):\n",
    "        with open(path+name, 'w') as f:\n",
    "            pickle.dump(self,f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "           \"XGBoost\" : XGBClassifier()\n",
    "}\n",
    "\n",
    "params = {'XGBoost':  {\"colsample_bytree\": [0.3,0.5,0.8,1],\"gamma\":[0,10,50,100],\n",
    "                        \"max_depth\": [2,4,6], # default 3\\\n",
    "                        \"n_estimators\": [50,100], # default 100\n",
    "                        \"subsample\": [0.3,0.5,0.8,1]}\n",
    "}\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_event = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_factor in np.arange(0.1,1.0,0.2):\n",
    "    models = {\n",
    "           \"XGBoost\" : XGBClassifier()}\n",
    "    print('------------------------------------------------')\n",
    "    print('Running for Weight Factor - ', weight_factor)\n",
    "    transform = MergedTransform(weight_factor=weight_factor)\n",
    "    wrapper_event = EstimatorSelectionHelperWrapper(models, params, y_label='majority_same_event')\n",
    "    wrapper_event.fit(transform.transform(train_df))\n",
    "    classifiers_event[weight_factor] = wrapper_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in classifiers_event:\n",
    "    print('====================================================')\n",
    "    print('Running for Weight Factor - ', key)\n",
    "    classifiers_event[key].predict(transform.transform(test_df))\n",
    "#     classifiers_event[key].save_models('../../data/models/models_mixed/','weighted_merge_'+str(key)+\"_snap_event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['majority_topic_1'] = train_df[train_df.columns[13:20]].idxmax(axis=1).str.split(\".\").str.get(-1)\n",
    "train_df['majority_topic_2'] = train_df[train_df.columns[20:27]].idxmax(axis=1).str.split(\".\").str.get(-1)\n",
    "train_df['majority_same_topic']=train_df['majority_topic_1'] == train_df['majority_topic_2']\n",
    "test_df['majority_topic_1'] = test_df[test_df.columns[13:20]].idxmax(axis=1).str.split(\".\").str.get(-1)\n",
    "test_df['majority_topic_2'] = test_df[test_df.columns[20:27]].idxmax(axis=1).str.split(\".\").str.get(-1)\n",
    "test_df['majority_same_topic']=test_df['majority_topic_1'] == test_df['majority_topic_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_event = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_factor in np.arange(0.1,1.0,0.2):\n",
    "    models = {\n",
    "           \"XGBoost\" : XGBClassifier()}\n",
    "    print('------------------------------------------------')\n",
    "    print('Running for Weight Factor - ', weight_factor)\n",
    "    transform = MergedTransform(weight_factor=weight_factor)\n",
    "    wrapper_event = EstimatorSelectionHelperWrapper(models, params, y_label='majority_same_topic')\n",
    "    wrapper_event.fit(transform.transform(train_df))\n",
    "    classifiers_event[weight_factor] = wrapper_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in classifiers_event:\n",
    "    print('====================================================')\n",
    "    print('Running for Weight Factor - ', key)\n",
    "    classifiers_event[key].predict(transform.transform(test_df))\n",
    "#     classifiers_event[key].save_models('../../data/models/models_mixed/','weighted_merge_'+str(key)+\"_snap_topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
